from langchain.llms.base import LLM
from typing import Any, List, Optional, Dict
import torch
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging
from pydantic import Field, BaseModel

logger = logging.getLogger(__name__)

class LocalLLM(LLM, BaseModel):
    
    model_name: str = Field(default="EleutherAI/gpt-neo-1.3B", description="æ¨¡å‹åç§°")
    device: str = Field(default="cuda", description="è¿è¡Œè®¾å¤‡")
    temperature: float = Field(default=0.7, description="ç”Ÿæˆæ¸©åº¦")
    tokenizer: Any = Field(default=None, description="åˆ†è¯å™¨")
    model: Any = Field(default=None, description="æ¨¡å‹")
    
    def __init__(self, **kwargs):
        """åˆå§‹åŒ–"""
        super().__init__(**kwargs)
        try:
            # åŠ è½½åˆ†è¯å™¨å’Œç”Ÿæˆæ¨¡å‹
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token  # è®¾ç½® pad_token ä¸º eos_token
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map="auto"
            ).to(self.device)
            logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹: {self.model_name}")
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            raise
    
    def _call(self, input_text: str) -> str:
        prompt = (
            "You are a classifier. Only respond with either 'CNN' or 'RNN'.\n"
            "Task: Recognize handwritten digits.\nAnswer: CNN\n"
            "Task: Identify objects in images.\nAnswer: CNN\n"
            "Task: Detect cats in pictures.\nAnswer: CNN\n"
            "Task: Forecast next month's sales from past data.\nAnswer: RNN\n"
            "Task: Predict future stock prices.\nAnswer: RNN\n"
            "Task: Model user behavior over time.\nAnswer: RNN\n"
            f"Task: {input_text}\nAnswer:"
        )
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True).to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=3,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # ğŸ”¥ åªå–ç”Ÿæˆçš„æ–°å†…å®¹éƒ¨åˆ†ï¼ˆå»æ‰åŸå§‹ prompt éƒ¨åˆ†ï¼‰
        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        result = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)

        # æ¸…æ´— & æå–é¦–è¯
        result = result.upper().strip().strip(".! ").split()[0]

        return result
    
    @property
    def _llm_type(self) -> str:
        """è¿”å›LLMç±»å‹"""
        return "local_llm"
    
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è¿”å›æ¨¡å‹å‚æ•°"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "temperature": self.temperature
        } 